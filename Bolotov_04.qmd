---
title: "Big Data - Practice 04"
author: "yehorbolt"
format:
  pdf:
    toc: true         
    number-sections: false
    geometry: margin=2.5cm
    fontsize: 12pt
    linestretch: 1.3   
    mainfont: "Georgia"
    colorlinks: true  
    linkcolor: blue
    urlcolor: teal
    citecolor: magenta
editor: visual
---

{{< pagebreak >}}

------------------------------------------------------------------------

## 1. Introduction and Setup

This analysis refines the log-log regression model developed previously.
The goal is to improve model stability and statistical validity using advanced techniques, including model selection based on the **Bayesian Information Criterion (BIC)**, in-depth diagnostics, and Principal Component Analysis (PCA) to address multicollinearity.

```{r setup-and-load, include=FALSE}
# Load required libraries
library(arrow)
library(dplyr)
library(ggplot2)
library(broom)
library(MASS) 
library(car)  
library(knitr) 
library(ggcorrplot)

Sys.setlocale("LC_TIME", "English")

# Load the cleaned taxi dataset
taxi_clean <- read_parquet("Data/taxi_clean_data.parquet")

# Create log-transformed variables for modeling
taxi_model_data <- taxi_clean %>%
  mutate(
    log_total = log(total_amount),
    log_distance = log(trip_distance),
    log_fare = log(fare_amount),
    log_tip = log(tip_amount + 1), 
    log_tolls = log(tolls_amount + 1) 
  )
```

---

## 2. Initial "Full" Model and Multicollinearity

We start with a "full" model including all potentially relevant predictors 
to check for multicollinearity using the Variance Inflation Factor (VIF).
A VIF score > 5 (and especially > 10) suggests high multicollinearity.

```{r full-model-vif, echo=TRUE, warning=FALSE}
# Fit the initial "full" model
full_model <- lm(log_total ~ log_distance + log_fare + log_tip + log_tolls + 
                   passenger_count + payment_type + pickup_hour 
                   + day_of_week + VendorID,
                 data = taxi_model_data)

# Print the VIF scores
vif_scores <- vif(full_model)
kable(vif_scores, caption="VIF Scores for Full Model", digits=2)
```

**Finding:** The VIF scores for `log_distance` (**~9.89**) and `log_fare` (**~10.26**) confirm significant multicollinearity.

To visualize this, we can plot a correlation matrix of the numeric predictors. The strong positive correlation (shown as ~0.95-0.97 in the plot) between `log_distance` and `log_fare` is the source of the high VIFs.

```{r full-model-corr-plot, echo=TRUE, warning=FALSE, fig.width=6, fig.height=5}
# Select only numeric predictors for correlation
numeric_vars <- taxi_model_data %>%
  dplyr::select(log_total, log_distance, log_fare, log_tip, log_tolls, 
  passenger_count, pickup_hour)

# Calculate correlation matrix
corr <- round(cor(numeric_vars), 2)

# Plot heatmap
ggcorrplot(corr, 
           hc.order = TRUE, 
           type = "lower",
           lab = TRUE, 
           lab_size = 3,
           title = "Correlation of Numeric Predictors")
```

---

## 3. Model Improvement I: Stepwise Selection (BIC)

Our first attempt to fix this is automated, using backward stepwise selection based on **BIC** (Bayesian Information Criterion).

```{r stepwise-bic-model, echo=TRUE, warning=FALSE}
# Determine n (number of observations) for BIC calculation
n_obs <- nrow(taxi_model_data)

# Run backward stepwise selection using BIC (k = log(n))
step_model_bic <- stepAIC(full_model, 
                         direction = "backward", 
                         trace = FALSE, 
                         k = log(n_obs)) 

# Show the summary of the final model selected by BIC
summary(step_model_bic)
```

**Interpretation:** Stepwise selection using BIC removed `VendorID`. However, **it kept both `log_distance` and `log_fare`**. This model has the highest `Adj. R-squared` (0.9717), but the multicollinearity problem has *not* been solved. The coefficients for distance and fare (e.g., `log_distance` coefficient is tiny/negative, `log_fare` is huge) are unstable and cannot be interpreted.

---

## 4. Model Improvement II: Manually Refined Model

The automated BIC model is uninterpretable. Let's try a pragmatic approach based on **domain knowledge**: remove `log_fare` and keep only `log_distance`. Our hypothesis is that `fare_amount` is just a function of `trip_distance`, so we won't lose much predictive power.

```{r manual-model, echo=TRUE, warning=FALSE}
# Fit the manual model, removing the redundant log_fare
manual_model <- lm(log_total ~ log_distance + log_tip + log_tolls + 
                     passenger_count + payment_type + pickup_hour + 
                     day_of_week + VendorID,
                   data = taxi_model_data)

# Show the summary
summary(manual_model)

# Check the VIF scores for this new model
vif_manual <- vif(manual_model)
kable(vif_manual, caption="VIF Scores for Manually Refined Model", digits=2)
```

**Interpretation: This model is a failure.**
Our hypothesis was wrong. By removing `log_fare`, the `Adj. R-squared` plummeted from **0.9717** (for the BIC model) to **0.907**.

This means `log_fare` contains critical predictive information (like base fees, peak-hour surcharges, or airport fees) that is *not* captured by `log_distance`. We have created a stable, interpretable model, but we have **sacrificed far too much predictive power**.

---

## 5. Model Improvement III: Principal Component Analysis (PCA)

We have a problem:
1. “BIC Model”: High accuracy, but uninterpretable. 
2. “Manual Model”: Interpretable, but low accuracy.

Let’s try a third method, PCA. This is a “black box” method that combines correlated predictors into uncorrelated “components.” It is **uninterpretable**, but it will solve the problem of multicollinearity, making the model stable.

```{r pca-prep, echo=TRUE, warning=FALSE}
# 1. Isolate the correlated numeric predictors
cor_vars <- taxi_model_data %>% 
  dplyr::select(log_distance, log_fare, log_tip, log_tolls)

# 2. Run PCA. We set scale. = TRUE to standardize the variables
pca_results <- prcomp(cor_vars, scale. = TRUE)

# Show how much variance each PC explains
summary(pca_results)
```

Now, fit models using these new, uncorrelated `PC` variables.

```{r pca-models, echo=TRUE, warning=FALSE}
# 3. Create a new dataset with the PCs
pca_data <- bind_cols(
  taxi_model_data %>% dplyr::select(-log_distance, -log_fare, -log_tip, -log_tolls),
  as.data.frame(pca_results$x)
)

# 4. Fit a new model using all 4 PCs
pca_model_full <- lm(log_total ~ PC1 + PC2 + PC3 + PC4 + 
                       passenger_count + payment_type + pickup_hour 
                       + day_of_week + VendorID,
                     data = pca_data)

# 5. Fit a reduced model (dropping PC4, which explains little)
pca_model_reduced <- lm(log_total ~ PC1 + PC2 + PC3 + 
                          passenger_count + payment_type + pickup_hour + 
                          day_of_week,
                        data = pca_data)
```

**Interpretation:** The `pca_model_full` will have an `Adj. R-squared` very close to the `BIC_model` (approx 0.9717), but its VIFs will all be 1. This is a stable, high-accuracy model, but it is completely uninterpretable.

---

## 6. Comparative Advanced Diagnostics

Standard diagnostic plots for the `BIC Model` and `Manual Model` just to confirm they "pass" standard checks.

```{r create-diagnostic-models, echo=FALSE, include=FALSE}
# Create a sample for fast diagnostics
set.seed(123)
diag_sample <- taxi_model_data %>% slice_sample(n = 10000)

# Re-fit the BIC model formula on the sample
step_model_diag_bic <- lm(formula(step_model_bic), data = diag_sample)

# Re-fit the MANUAL model formula on the sample
manual_model_diag <- lm(formula(manual_model), data = diag_sample)
```

```{r advanced-diagnostics-bic, echo=TRUE, message=FALSE, warning=FALSE, fig.width=10, fig.height=8, fig.cap="Standard Diagnostics (BIC Model)"}
# Plot diagnostics in a 2x2 grid for BIC model
par(mfrow = c(2, 2))
plot(step_model_diag_bic, which = 1)
plot(step_model_diag_bic, which = 2)
plot(step_model_diag_bic, which = 4)
plot(step_model_diag_bic, which = 5)
par(mfrow = c(1, 1))
```

```{r advanced-diagnostics-manual, echo=TRUE, message=FALSE, warning=FALSE, fig.width=10, fig.height=8, fig.cap="Standard Diagnostics (Manual Model)"}
# Plot diagnostics in a 2x2 grid for MANUAL model
par(mfrow = c(2, 2))
plot(manual_model_diag, which = 1) 
plot(manual_model_diag, which = 2) 
plot(manual_model_diag, which = 4) 
plot(manual_model_diag, which = 5) 
par(mfrow = c(1, 1))
```

**Diagnostic Interpretation:** The plots look "clean" for both models. This is the key takeaway: **Standard residual diagnostics do not reveal a multicollinearity problem** or a massive performance loss problem.

---

## 7. Final Model Comparison

```{r model-comparison-final, echo=FALSE, message=FALSE, warning=FALSE}
# Extract Summaries
summary_bic <- summary(step_model_bic)
summary_manual <- summary(manual_model)
summary_pca_full <- summary(pca_model_full)
summary_pca_reduced <- summary(pca_model_reduced)

# Statistics Table
model_stats_final <- data.frame(
  Model = c("Stepwise (BIC)", "Manual (No Fare)", "PCA (Full)", "PCA (Reduced)"),
  Num_Predictors = c(
    length(coef(step_model_bic)) - 1,
    length(coef(manual_model)) - 1,
    length(coef(pca_model_full)) - 1,
    length(coef(pca_model_reduced)) - 1
  ),
  Adj_R_Squared = c(
    summary_bic$adj.r.squared,
    summary_manual$adj.r.squared,
    summary_pca_full$adj.r.squared,
    summary_pca_reduced$adj.r.squared
  ),
  RSE = c(
    summary_bic$sigma,
    summary_manual$sigma,
    summary_pca_full$sigma,
    summary_pca_reduced$sigma
  ),
  Solves_Multicollinearity = c("No", "Yes", "Yes", "Yes"),
  Interpretability = c("Flawed / None", "High", "Very Low", "Very Low")
)

# Print a clean table
kable(model_stats_final, 
      caption = "Comparison of Final Models", 
      digits = c(NA, 0, 4, 4, NA, NA),
      col.names = c("Model", "# Predictors", "Adj. R²", "RSE", 
                    "Solves Multicollinearity?", "Interpretability")) 

```

### Summary of Results and Interpretation

The analysis reveals a complex but common trade-off. We do not have a single "best" model, but rather a choice based on our goal:

* **`Stepwise Model (BIC)`:** This model provides the **highest predictive accuracy** (`Adj. R² = 0.9717`). However, its coefficients are unstable and uninterpretable due to high multicollinearity.
* **`Manual Model (No Fee)`:** This is the only model that can be interpreted. However, its accuracy is **unacceptably low** (`Adj. R² = 0.907`). This model is rejected.
* **`PCA (Full)`:** This model achieves **the same high accuracy** as the BIC model (`Adj. R² = 0.9717`) and also **resolves multicollinearity** (VIF = 1). Its weakness is that it is a "black box" and **has no interpretability**.

**Final choice:**
* **For pure, one-time prediction:** The `Stepwise (BIC)` model is fine.
* **For an interpretable model:** We could not find a model with high accuracy.
* **For a stable, reliable production model (e.g. in a data processing application):** The **`PCA (Full)`** model is the most reliable choice. It combines high accuracy with statistical stability, even if we sacrifice interpretability.

## 8. Problems encountered

The main problem was the strong multicollinearity between `log_distance` and `log_fare`. Our analysis showed that this was not a simple case of redundancy. `log_fare` adds significant predictive power, probably due to the base fees and additional fees. This forced us to find a trade-off between:

1. Accuracy (BIC model) 
2. Interpretability (manual model) 
3. Stability (PCA model) 

We were unable to achieve all three. This analysis shows that the application of domain knowledge (`manual model') should always be tested against the data; our initial assumption was wrong, and the data confirmed it.

## 9. Exam-style questions and answers

**Question:** An analyst uses a backward stepwise regression based on BIC and finds that the final model retains two variables with very high VIFs (>10). Should the analyst accept this model? Explain why or not.

**Answer:** The analyst should be **cautious** about accepting this model, and the answer depends on the purpose:

* **Why BIC retained them:** BIC selected this model because *both* variables provided a statistically significant improvement in *fit* (predictive power) that outweighed the complexity penalty.
* **Problem:** High VIFs (>4) mean that the coefficients for these two variables are unstable and have greatly inflated standard errors. This makes their individual p-values ​​and coefficient estimates unreliable and uninterpretable.
* **Conclusion:**
* The model is **acceptable for PREDICTION**. High VIF values ​​do not affect the overall predictive accuracy of the model (which is why BIC chose it).
* The model is **NOT ACCEPTABLE for INTERPRETATION**. We cannot trust the model to explain the *individual effect* of any variable.



